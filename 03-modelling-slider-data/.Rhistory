print(summary_table)
# Encode factors and levels
df$target <- factor(df$target)
df$distractor <- factor(df$distractor)
# Show levels of factors
levels(df$target)
levels(df$distractor)
# Calculate mean, sd, se of right-bounded reading times per condition (3 x 4 = 12 data points)
df_summarised <- df %>% group_by(target, distractor) %>% summarise(mean_rt = mean(readingtime), sd_rt = sd(readingtime), se_rt = sd_rt/sqrt(n()), .groups = "drop")
# Show summarised data
df_summarised
# Table 1: Export summarised data to a Latex table
print(xtable(df_summarised), type = "latex")
# Figure 1: Plot a theory-driven prediction plot
## Generate dummy data
df_dummy <- data.frame(
target = factor(rep(c("match", "mismatch"), each = 2)),
distractor = factor(rep(c("match", "mismatch"), times = 2)),
mean_rt = c(300, 350, 380, 360)
)
# Customise theme
my_theme <- function() {
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.title.x = element_text(size = 25, margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.title.y = element_text(size = 22, margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.x = element_text(size = 25, angle = 45, hjust = 1),
axis.text.y = element_text(size = 20),
legend.title = element_text(size = 20),
legend.text = element_text(size = 20),
strip.text.x = element_text(size = 25),
strip.text.y = element_text(size = 25, angle = 0),
panel.grid.major = element_line(colour = "grey90", size = 0.25),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
}
## Plot interaction with dummy data
figure1 <- ggplot(df_dummy, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
labs(title = "Interaction Plot of Theory-Driven Prediction with Dummy Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure1.pdf", figure1, width = 10, height = 8)
# Figure 2: Plot mean reading times per condition with error bars
figure2 <- ggplot(df_summarised, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
geom_errorbar(aes(ymin = mean_rt - se_rt, ymax = mean_rt + se_rt), width = 0.1) +
labs(title = "Interaction Plot of Empirical Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure2.pdf", figure2, width = 10, height = 8)
# Run normality check on raw RTs before fitting linear models
## Create a qqnorm plot to visualise data with 45-degree line
qqnorm(df$readingtime, main = "Normal Q-Q Plot of Raw RTs")
qqline(df$readingtime, col = "red")  # Adds a 45-degree line in red for reference
## There are many outliers above the reference line, so RTs are right-skewed, not normally distributed
# log-transform RTs
df$log_rt <- log(df$readingtime)
# Re-run normality check on log-transformed RTs
# Create a qqnorm plot to visualise data with 45-degree line
qqnorm(df$log_rt, main = "Normal Q-Q Plot of Log-Transformed RTs")
qqline(df$log_rt, col = "red")  # Adds a 45-degree line in red for reference
# We observe that the log-transformed RTs are more normally distributed than the raw RTs
# Fit a linear mixed-effects model with log_rt
## Maximal random structures:
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target * distractor|item), data = df)
## Reduce random structures stepwise:
### First on the item level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target + distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (distractor|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (1|item), data = df)
## Then on the subject level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (distractor|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (1|subj) + (target * distractor|item), data = df)
## Both reduced to random intercept
model_coverged_1 <- lmer(log_rt ~ target * distractor + (1|subj) + (1|item), data = df)
# This is the first converged model with maximal random effect structure. Use this model for further analysis
# Use model_coverged_4 as the final model
model_full <- model_coverged_1
summary(model_full)
# Use log-likelihood ratio tests to perform hypothesis testing
## Compare model_full with a model without the interaction term
model_no_interaction <- update(model_full, . ~ . - target:distractor)
anova(model_full, model_no_interaction) # interaction term is significant
## Explanation: The significant two-way interaction is due to the fact that the distractor mismatch condition has different effects on the reading times of the target words under different target conditions. If target is match, the distractor mismatch will lead to a facilitory interference, namely decreasing of RTs. If target is mismatch, the distractor mismatch will lead to an inhibitory interference, namely increasing of RTs.
df_target_match <- subset(df, target == "match")
## isSingular:(distractor|subj) + (distractor|item)
model_target_match_w.distractor <- lmer(log_rt ~ distractor +
(1|subj) +
(1|item), data = df_target_match)
summary(model_target_match_w.distractor)
model_target_match_w.o.distractor <- update(model_target_match_w.distractor, . ~ . - distractor)
anova(model_target_match_w.distractor, model_target_match_w.o.distractor) # no sig. main effect of distractor with target match
# beta = -0.02936,  se =  0.03236, t =  -0.907, p = 0.365
# The distractor mismatch condition lead to decrease of RT when target condition is match. However, the effect is not significant.
df_target_mismatch <- subset(df, target == "mismatch")
model_target_mismatch_w.distractor <- lmer(log_rt ~ distractor +
(1|subj) +
(1|item), data = df_target_mismatch)
summary(model_target_mismatch_w.distractor)
model_target_mismatch_w.o.distractor <- update(model_target_mismatch_w.distractor, . ~ . - distractor)
anova(model_target_mismatch_w.distractor, model_target_mismatch_w.o.distractor) # marginal main effect of distractor with target match
# beta = 0.05832; se = 0.03154; t = 1.849;  p =  0.0648
# The distractor mismatch condition lead to increase of RT when target condition is also mismatch.
# Fit a linear mixed-effects model with log_rt
## Maximal random structures:
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target * distractor|item), data = df)
## Reduce random structures stepwise:
### First on the item level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target + distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (distractor|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (1|item), data = df)
## Then on the subject level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (distractor|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (1|subj) + (target * distractor|item), data = df)
# Change level order
df$distractor <- factor(df$distractor, levels = c("mismatch", "match"))
## Both reduced to random intercept
model_coverged_1 <- lmer(log_rt ~ target * distractor + (1|subj) + (1|item), data = df)
# This is the first converged model with maximal random effect structure. Use this model for further analysis
# Import relevant modules
library(ggplot2)
library(dplyr)
library(xtable)
library(lme4)
library(lmerTest)
# Import data set
df <- read.csv("data-task.csv")
# Take a look at the data set
glimpse(df)
# Show numbers of subjects and items
n_subj <- length(unique(df$subj))
n_item <- length(unique(df$item))
cat("Number of subjects:", n_subj, "\n")
cat("Number of items:", n_item, "\n")
# Why 38 items in a 2x2 design? How many experimental conditions? And how many filler conditions?
# Also 56 * 38 = 2128 data points, but we have here only 2084 data points in data set. Are there missing values?
summary_table <- df %>%
group_by(subj) %>%
summarise(Items_Seen = n())
print(summary_table)
# Encode factors and levels
df$target <- factor(df$target)
df$distractor <- factor(df$distractor)
# Show levels of factors
levels(df$target)
levels(df$distractor)
# Calculate mean, sd, se of right-bounded reading times per condition (3 x 4 = 12 data points)
df_summarised <- df %>% group_by(target, distractor) %>% summarise(mean_rt = mean(readingtime), sd_rt = sd(readingtime), se_rt = sd_rt/sqrt(n()), .groups = "drop")
# Show summarised data
df_summarised
# Table 1: Export summarised data to a Latex table
print(xtable(df_summarised), type = "latex")
# Figure 1: Plot a theory-driven prediction plot
## Generate dummy data
df_dummy <- data.frame(
target = factor(rep(c("match", "mismatch"), each = 2)),
distractor = factor(rep(c("match", "mismatch"), times = 2)),
mean_rt = c(300, 350, 380, 360)
)
# Customise theme
my_theme <- function() {
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.title.x = element_text(size = 25, margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.title.y = element_text(size = 22, margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.x = element_text(size = 25, angle = 45, hjust = 1),
axis.text.y = element_text(size = 20),
legend.title = element_text(size = 20),
legend.text = element_text(size = 20),
strip.text.x = element_text(size = 25),
strip.text.y = element_text(size = 25, angle = 0),
panel.grid.major = element_line(colour = "grey90", size = 0.25),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
}
## Plot interaction with dummy data
figure1 <- ggplot(df_dummy, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
labs(title = "Interaction Plot of Theory-Driven Prediction with Dummy Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure1.pdf", figure1, width = 10, height = 8)
# Figure 2: Plot mean reading times per condition with error bars
figure2 <- ggplot(df_summarised, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
geom_errorbar(aes(ymin = mean_rt - se_rt, ymax = mean_rt + se_rt), width = 0.1) +
labs(title = "Interaction Plot of Empirical Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure2.pdf", figure2, width = 10, height = 8)
# Run normality check on raw RTs before fitting linear models
## Create a qqnorm plot to visualise data with 45-degree line
qqnorm(df$readingtime, main = "Normal Q-Q Plot of Raw RTs")
qqline(df$readingtime, col = "red")  # Adds a 45-degree line in red for reference
## There are many outliers above the reference line, so RTs are right-skewed, not normally distributed
# log-transform RTs
df$log_rt <- log(df$readingtime)
# Re-run normality check on log-transformed RTs
# Create a qqnorm plot to visualise data with 45-degree line
qqnorm(df$log_rt, main = "Normal Q-Q Plot of Log-Transformed RTs")
qqline(df$log_rt, col = "red")  # Adds a 45-degree line in red for reference
# We observe that the log-transformed RTs are more normally distributed than the raw RTs
# Fit a linear mixed-effects model with log_rt
## Maximal random structures:
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target * distractor|item), data = df)
## Reduce random structures stepwise:
### First on the item level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target + distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (distractor|item), data = df)
### isSingular, fail to converge: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (1|item), data = df)
## Then on the subject level
### isSingular: lmer(log_rt ~ target * distractor + (target * distractor|subj) + (target|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (target|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (distractor|subj) + (target * distractor|item), data = df)
### isSingular: lmer(log_rt ~ target * distractor + (1|subj) + (target * distractor|item), data = df)
# Change level order
df$distractor <- factor(df$distractor, levels = c("mismatch", "match"))
## Both reduced to random intercept
model_coverged_1 <- lmer(log_rt ~ target * distractor + (1|subj) + (1|item), data = df)
# This is the first converged model with maximal random effect structure. Use this model for further analysis
# Use model_coverged_4 as the final model
model_full <- model_coverged_1
summary(model_full)
# Use log-likelihood ratio tests to perform hypothesis testing
## Compare model_full with a model without the interaction term
model_no_interaction <- update(model_full, . ~ . - target:distractor)
anova(model_full, model_no_interaction) # interaction term is significant
## Explanation: The significant two-way interaction is due to the fact that the distractor mismatch condition has different effects on the reading times of the target words under different target conditions. If target is match, the distractor mismatch will lead to a facilitory interference, namely decreasing of RTs. If target is mismatch, the distractor mismatch will lead to an inhibitory interference, namely increasing of RTs.
df_target_match <- subset(df, target == "match")
## isSingular:(distractor|subj) + (distractor|item)
model_target_match_w.distractor <- lmer(log_rt ~ distractor +
(1|subj) +
(1|item), data = df_target_match)
summary(model_target_match_w.distractor)
model_target_match_w.o.distractor <- update(model_target_match_w.distractor, . ~ . - distractor)
anova(model_target_match_w.distractor, model_target_match_w.o.distractor) # no sig. main effect of distractor with target match
# beta = -0.02936,  se =  0.03236, t =  -0.907, p = 0.365
# The distractor mismatch condition lead to decrease of RT when target condition is match. However, the effect is not significant.
df_target_mismatch <- subset(df, target == "mismatch")
model_target_mismatch_w.distractor <- lmer(log_rt ~ distractor +
(1|subj) +
(1|item), data = df_target_mismatch)
summary(model_target_mismatch_w.distractor)
model_target_mismatch_w.o.distractor <- update(model_target_mismatch_w.distractor, . ~ . - distractor)
anova(model_target_mismatch_w.distractor, model_target_mismatch_w.o.distractor) # marginal main effect of distractor with target match
# beta = 0.05832; se = 0.03154; t = 1.849;  p =  0.0648
# The distractor mismatch condition lead to increase of RT when target condition is also mismatch.
# Figure 1: Plot a theory-driven prediction plot
## Generate dummy data
df_dummy <- data.frame(
target = factor(rep(c("mismatch", "match"), each = 2)),
distractor = factor(rep(c("mismatch", "match"), times = 2)),
mean_rt = c(300, 350, 380, 360)
)
# Customise theme
my_theme <- function() {
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.title.x = element_text(size = 25, margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.title.y = element_text(size = 22, margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.x = element_text(size = 25, angle = 45, hjust = 1),
axis.text.y = element_text(size = 20),
legend.title = element_text(size = 20),
legend.text = element_text(size = 20),
strip.text.x = element_text(size = 25),
strip.text.y = element_text(size = 25, angle = 0),
panel.grid.major = element_line(colour = "grey90", size = 0.25),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
}
## Plot interaction with dummy data
figure1 <- ggplot(df_dummy, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
labs(title = "Interaction Plot of Theory-Driven Prediction with Dummy Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure1.pdf", figure1, width = 10, height = 8)
# Figure 1: Plot a theory-driven prediction plot
## Generate dummy data
df_dummy <- data.frame(
target = factor(rep(c("match", "mismatch"), each = 2)),
distractor = factor(rep(c("mismatch", "match"), times = 2)),
mean_rt = c(300, 350, 380, 360)
)
# Customise theme
my_theme <- function() {
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.title.x = element_text(size = 25, margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.title.y = element_text(size = 22, margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.x = element_text(size = 25, angle = 45, hjust = 1),
axis.text.y = element_text(size = 20),
legend.title = element_text(size = 20),
legend.text = element_text(size = 20),
strip.text.x = element_text(size = 25),
strip.text.y = element_text(size = 25, angle = 0),
panel.grid.major = element_line(colour = "grey90", size = 0.25),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
}
## Plot interaction with dummy data
figure1 <- ggplot(df_dummy, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
labs(title = "Interaction Plot of Theory-Driven Prediction with Dummy Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure1.pdf", figure1, width = 10, height = 8)
# Figure 1: Plot a theory-driven prediction plot
## Generate dummy data
df_dummy <- data.frame(
target = factor(rep(c("match", "mismatch"), each = 2)),
distractor = factor(rep(c("mismatch", "match"), times = 2)),
mean_rt = c(300, 350, 380, 360)
)
# Customise theme
my_theme <- function() {
theme_minimal() +
theme(
plot.title = element_text(size = 20, face = "bold"),
axis.title.x = element_text(size = 25, margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.title.y = element_text(size = 22, margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.x = element_text(size = 25, angle = 45, hjust = 1),
axis.text.y = element_text(size = 20),
legend.title = element_text(size = 20),
legend.text = element_text(size = 20),
strip.text.x = element_text(size = 25),
strip.text.y = element_text(size = 25, angle = 0),
panel.grid.major = element_line(colour = "grey90", size = 0.25),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
}
## Plot interaction with dummy data
figure1 <- ggplot(df_dummy, aes(x = target, y = mean_rt, group = distractor, shape = distractor, color = distractor)) +
geom_line() +
geom_point(size = 10) +
labs(title = "Interaction Plot of Theory-Driven Predictions with Dummy Data",
x = "Target Condition",
y = "Mean Reading Times (RTs)",
color = "Distractor Condition",
shape = "Distractor Condition") +
my_theme()
# Export figure to a pdf file
ggsave("plots/figure1.pdf", figure1, width = 10, height = 8)
library(ggplot2)
# Load the data
data <- read.csv("../posterior_samples/production_posteriorPredictive_test4_empiricalNone")
setwd("~/")
setwd("~/Documents/GitHub/numpyro_adjective_modelling/03-modelling-slider-data")
library(ggplot2)
# Load the data
data <- read.csv("../posterior_samples/production_posteriorPredictive_test4_empiricalNone")
library(ggplot2)
# Load the data
data <- read.csv("../posterior_samples/production_posteriorPredictive_test4_empiricalNone")
library(ggplot2)
# Load the data
data <- read.csv("../posterior_samples/production_posteriorPredictive_test4_empiricalNone.csv")
# correlation plot
ggplot(data, aes(x = annotation_encoded, y = mean_predictions_map)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
View(data)
# correlation plot
ggplot(data, aes(x = annotation_encoded, y = mean_predictions)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
lm(data$mean_predictions ~ data$annotation_encoded)
rsq(data$mean_predictions ~ data$annotation_encoded)
cor.test(data$mean_predictions ~ data$annotation_encoded)
cor.test(data$mean_predictions, data$annotation_encoded)
cor.test(data$mean_predictions, data$annotation_encoded)
# correlation plot
ggplot(data, aes(x = annotation_encoded, y = mean_predictions, color = conditions)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
# correlation plot
data %>%
group_by(sharpness, combination, relevant_property) %>%
summarise(mean_predictions = mean(predictions),
mean_annotations = mean(annotation_encoded)) -> data_aggregated
library(ggplot2)
library(dplyr)
# Load the data
data <- read.csv("../posterior_samples/production_posteriorPredictive_test4_empiricalNone.csv")
# correlation plot
data %>%
group_by(sharpness, combination, relevant_property) %>%
summarise(mean_predictions = mean(predictions),
mean_annotations = mean(annotation_encoded)) -> data_aggregated
View(data)
# correlation plot
data %>%
group_by(sharpness, combination, relevant_property) %>%
summarise(mean_predictions_by_condition = mean(mean_predictions),
mean_annotations = mean(annotation_encoded)) -> data_aggregated
View(data_aggregated)
ggplot(data, aes(x = annotation_encoded, y = mean_predictions, color = conditions)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
ggplot(data_aggregated, aes(x = mean_annotations, y = mean_predictions_by_condition, color = conditions)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
ggplot(data_aggregated, aes(x = mean_annotations, y = mean_predictions_by_condition, color = combination)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
ggplot(data_aggregated, aes(x = mean_annotations, y = mean_predictions_by_condition, color = combination)) +
geom_point() +
facet_wrap(~sharpness) +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
ggplot(data_aggregated, aes(x = mean_annotations, y = mean_predictions_by_condition, color = relevant_property)) +
geom_point() +
facet_wrap(combination~sharpness) +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
View(data_aggregated)
# correlation plot
data %>%
group_by(sharpness, combination, relevant_property) %>%
summarise(mean_predictions_by_condition = mean(mean_predictions),
mean_annotations = mean(annotation_encoded)) %>%
mutate(n = nrow())-> data_aggregated
# correlation plot
data %>%
group_by(sharpness, combination, relevant_property) %>%
summarise(mean_predictions_by_condition = mean(mean_predictions),
mean_annotations = mean(annotation_encoded)) %>%
mutate(n = n())-> data_aggregated
ggplot(data_aggregated, aes(x = mean_annotations, y = mean_predictions_by_condition)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Correlation between annotation and mean predictions",
x = "Annotation",
y = "Mean predictions")
cor.test(data_aggregated$mean_predictions_by_condition, data_aggregated$mean_annotations)
cor.test(data_aggregated$mean_predictions_by_condition, data_aggregated$mean_annotations)
?cor.test
cor.test(data_aggregated$mean_predictions_by_condition, data_aggregated$mean_annotations, alternative = "greater")
